{
 "metadata": {
  "name": "",
  "signature": "sha256:d36f8bb38e36d7f12996f16fd48d2ecda0aa858bd8d4491244a6e7b9510dd91e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This script currently:\n",
      "\n",
      "* Reads a CSV file into a pandas data frame\n",
      "* Searches for HXL like hashtags to indicate the nature of columns (map_tags)\n",
      "  * If it finds them, if uses them as column headings\n",
      "  * If it does not find them, it camelCases the current heading\n",
      "  \n",
      "It then reads through the dataframe, using a RDF ontology to identify Classes, known properties, and relationships between those Classes. \n",
      "\n",
      "* If it finds a #tag that relates to a class, it creates an entity for that tag\n",
      "* If it finds a #tag that relates to a property, it attaches that property to the last entity created\n",
      "* If it finds a hierachical tag, such as #participant+company it creates both the relevant entities\n",
      "* It looks for any relevant relationships allowed by the ontology between parent and child in this hierachy, and creates those\n",
      "* If it finds unknown column headings, it mints an appropriate DataProperty and records the value (so that data is not thrown away)\n",
      "\n",
      "It has special handling for the +identifier attribute. \n",
      "\n",
      "* If creating #entity, and there is an #entitity+identifier column, use the value of that column to create an ID, otherwise, follow a class-specific pattern to mint an identifier\n",
      "\n",
      "ToDo:\n",
      "\n",
      "* Improving naming algorithm\n",
      "* Add provenance information relating each entity to the row it was drawn from, as well as the date and time at which assertions were known to be true. \n",
      "* Add special handling for properties such as Country etc.\n",
      "* Add handling for HXL language tags (two digit language codes such as +en)\n",
      "* ~~Add caching of identifiers (e.g. if encountering the same company name again, re-use the identifier assigned earlier)~~\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import uuid64\n",
      "from collections import OrderedDict, defaultdict\n",
      "from countrycode import countrycode\n",
      "from rdflib import Graph, URIRef, Literal\n",
      "from rdflib.namespace import FOAF, RDF, SKOS, OWL, RDFS\n",
      "from rdflib.namespace import Namespace\n",
      "import random\n",
      "prov = Namespace('http://www.w3.org/ns/prov#')\n",
      "ontology = 'http://resourceprojects.org/def/'\n",
      "base_uri = 'http://resourceprojects.org/'\n",
      "\n",
      "def clean_string(string):\n",
      "    invalid_chars = ''.join(c for c in map(chr, range(256)) if not c.isalnum())\n",
      "    return str(string).translate(None,invalid_chars)\n",
      "\n",
      "## Function to rename a pandas data frame based on #tags in the first 25 rows\n",
      "def map_tags(data):   \n",
      "    mapping = OrderedDict()\n",
      "    for line, row in data[0:25].iterrows():\n",
      "        tag_count = 0\n",
      "        for key in row.keys():\n",
      "            if str(row[key])[0] == \"#\":\n",
      "                mapping[key] = row[key]\n",
      "                tag_count +=1\n",
      "            else:\n",
      "                new_key = clean_string(key.title())\n",
      "                mapping[key] = new_key[0].lower()+new_key[1:]\n",
      "        if tag_count > 3: # We assume we've found the tag row once we've got a row with more than 3 tags in\n",
      "            #Remove the tag row\n",
      "            data = data.drop(line)\n",
      "            break         \n",
      "\n",
      "    if(tag_count < 3):\n",
      "        print \"No column tagging found in first 25 rows.\"\n",
      "    else:\n",
      "        data = data.rename(columns=mapping)\n",
      "    return data\n",
      "\n",
      "country_cache = {}\n",
      "def get_country(row):\n",
      "    if \"#country+code\" in row.keys():\n",
      "        country = row['#country+code']\n",
      "    elif \"#country\" in row.keys():\n",
      "        if row.get('#country',\"xx\") in country_cache.keys():\n",
      "            country = country_cache[row.get('#country',\"xx\")]\n",
      "        else:\n",
      "            country = countrycode(codes=[row.get('#country',\"\")],origin='country_name',target=\"iso2c\")[0]\n",
      "            country_cache[row.get('#country',\"xx\")] = country\n",
      "    else:\n",
      "        country = \"xx\"\n",
      "    return country.lower()\n",
      "\n",
      "\n",
      "   \n",
      "    \n",
      "\n",
      "def generate_project_identifier(name):\n",
      "    \"\"\"Generates a project identifier.\n",
      "    \n",
      "    If the project name is a single word, use the first 4 digits, then a 6 characther random alphanumeric string.\n",
      "    If the project name is two words, use the first two digits of each word. \n",
      "    \n",
      "    Uses clean_string to strip non alphanumeric ascii characters before processing. \n",
      "    \n",
      "    \"\"\"\n",
      "    name = name.lower().split(\" \")\n",
      "    if len(name) == 1:\n",
      "        start = clean_string(name[0])[:4]\n",
      "    else:\n",
      "        start = clean_string(name[0])[:2] + clean_string(name[1])[:2]\n",
      "    \n",
      "    \n",
      "    suffix = ''.join(random.choice('0123456789abcdefghijklmnopqrstuvwxyz') for i in range(6))\n",
      "    return start + \"-\" + suffix\n",
      "\n",
      "\n",
      "id_cache = {}\n",
      "def generate_identifier(path,row,country = \"xx\"):\n",
      "    if \"#\" + path + \"+identifier\" in row.keys(): #Check if this entity already has an identifier given in a column\n",
      "        if not row[\"#\" + path + \"+identifier\"].strip() == \"\":\n",
      "            return row[\"#\" + path + \"+identifier\"].strip()\n",
      "    \n",
      "    # If not, we need to work out what kind of entity we are dealing with, and then generate an identifier\n",
      "    entity = path.split(\"+\")[-1]\n",
      "    \n",
      "    if \"#\"+path in row.keys(): # Is there a column that might contain a name (e.g. for #participant+company+identifier do we have a #participant+company column we can use?)\n",
      "        cache_key = country + entity + clean_string(row[\"#\"+path])\n",
      "        if cache_key in id_cache.keys() and len(clean_string(row[\"#\"+path]).strip()) > 1:\n",
      "            return id_cache[cache_key]\n",
      "\n",
      "        if entity == \"project\":\n",
      "            identifier = country + \"/\" + generate_project_identifier(row[\"#\"+path])\n",
      "        elif entity == \"country\":\n",
      "            identifier = get_country(row).upper()\n",
      "        else:\n",
      "            identifier = country + \"/\" + uuid64.hex()\n",
      "\n",
      "        id_cache[cache_key] = identifier\n",
      "    else: # We had nothing to work with, so just general a UUID\n",
      "        identifier = country + \"/\" + uuid64.hex()\n",
      "  \n",
      "    return identifier\n",
      "\n",
      "\n",
      "def get_tag_type(tag,onto):\n",
      "    if ( URIRef(ontology+tag.title()), RDF.type, OWL.Class ) in onto:\n",
      "        return \"Class\"\n",
      "    elif ( URIRef(ontology+tag), RDF.type, OWL.ObjectProperty ) in onto:\n",
      "        return \"ObjectProperty\"\n",
      "    elif ( URIRef(ontology+tag), RDF.type, OWL.DatatypeProperty ) in onto:\n",
      "        return \"DataProperty\"\n",
      "    else: \n",
      "        # print \"Unknown tag: \" + tag\n",
      "        return \"Unknown\"\n",
      "\n",
      "# Create a new entity of a given class (entity_type)\n",
      "def create_entity(g,entity_type,path,row):\n",
      "    identifier = generate_identifier(entity_type,row,get_country(row))\n",
      "    entity = URIRef(base_uri+ entity_type.title()+\"/\"+identifier )\n",
      "    g.add((entity,RDF.type,URIRef(ontology+entity_type.title())))\n",
      "    if path in row.keys(): # Check if this tag exists along (i.e. not with attributes) and if so, use this for the name\n",
      "        g.add((entity,SKOS.prefLabel,Literal(row[path])))\n",
      "        \n",
      "    return entity\n",
      "\n",
      "# For entities a and b establish a relationship between them, based on the relationships specified in the ontology\n",
      "def create_relationship(g,a,b,onto):\n",
      "    a_class = g.value(a, RDF.type)\n",
      "    b_class = g.value(b,RDF.type)\n",
      "    query = \"\"\"PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
      "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
      "    PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
      "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "    SELECT DISTINCT ?rel WHERE {\n",
      "    ?rel a owl:ObjectProperty.\n",
      "    ?rel rdfs:domain <%s>.\n",
      "    ?rel rdfs:range <%s>.}\"\"\"%(a_class, b_class)\n",
      "    q = onto.query(query)\n",
      "    if len(q) > 1:\n",
      "        print \"Multiple relationships between \" + str(a_class) + \" and \" + str(b_class) + \" found. Check ontology\"\n",
      "\n",
      "    for res in q:\n",
      "        g.add((a,res['rel'],b))\n",
      "    \n",
      "    return None\n",
      "    \n",
      "\n",
      "# Expects a pandas data frame\n",
      "def generate_graph(data,onto,filename=\"unknown-file\"):\n",
      "    g = Graph()\n",
      "    data = data.fillna(\"\") # Make sure we set any NA values to blank\n",
      "    ## Remove row limit when in production\n",
      "    for line, row in data[0:300].iterrows():\n",
      "        #First we create the row entity.\n",
      "        entity = URIRef(base_uri + \"sources/\"+filename+\"/row/\"+ str(line))\n",
      "        g.add((entity,RDF.type,URIRef(ontology + \"Row\")))\n",
      "        ## ToDo: Add more provenance information here\n",
      "        row_entity = entity ##Keep track of the row entity. \n",
      "        object_cache = {}\n",
      "        parent = None\n",
      "        \n",
      "        for key in row.keys(): ## Right now we assume that tags are well ordered. I.e. #company+code does not come before #company\n",
      "            tag_path = key.split(\"+\")\n",
      "            top = tag_path[0]\n",
      "            # if path[0] is a Class, check it has been created.\n",
      "            # if path[0] is a property, apply it to the last known entity\n",
      "            current_path = \"\"\n",
      "            \n",
      "            # parent = None  ## Right now we're establising any possible relationships between entities identified, according to the ontology. ToDo: Review this and adapt.\n",
      "            \n",
      "            tag_level = 1\n",
      "            for tag in tag_path:\n",
      "                current_path += \"+\"+tag if not current_path == \"\" else tag # Reconstruct the path if we're getting deeper in...\n",
      "                if tag[0] == \"#\":\n",
      "                    tag = tag[1:]\n",
      "                    \n",
      "                tag_type = get_tag_type(tag,onto)\n",
      "                if tag_type == \"Class\":\n",
      "                    if not tag.title() in object_cache:\n",
      "                        entity = create_entity(g,tag,current_path,row)\n",
      "                        g.add((entity,prov.wasDerivedFrom,row_entity))\n",
      "                        object_cache[tag.title()] = {\"entity\":entity,\"level\":tag_level}\n",
      "                    else:\n",
      "                        entity = object_cache[tag.title()]['entity']\n",
      "                    \n",
      "                    ## If there is a parent we are aware of, then we need to relate this node to its parent.\n",
      "                    if not parent == None:\n",
      "                        if not(parent == entity):\n",
      "                            create_relationship(g,parent,entity,onto)\n",
      "                            create_relationship(g,entity,parent,onto)\n",
      "                            pass\n",
      "                            \n",
      "                    parent = entity # We need to check at each loop if there is a parent we should be relating too\n",
      "                elif tag_type == \"ObjectProperty\":\n",
      "                    pass\n",
      "                elif tag_type == \"DataProprerty\": #We're dealing with a data property (ToDo: Handle for range validation of data properties here)\n",
      "                    g.add((entity,URIRef(ontology+tag),Literal(row[key])))\n",
      "                elif tag==\"identifier\":\n",
      "                    g.add((entity,SKOS.notation,Literal(row[key])))\n",
      "                else:\n",
      "                    g.add((entity,URIRef(ontology+\"misc/\"+ tag),Literal(row[key])))\n",
      "                    \n",
      "                tag_level += 1\n",
      "        \n",
      "        for ent in object_cache:\n",
      "            if object_cache[ent]['level'] == 1:\n",
      "                entity = object_cache[ent]['entity']\n",
      "                for rel in object_cache:\n",
      "                    if object_cache[rel]['level'] == 1:\n",
      "                        related = object_cache[rel]['entity']\n",
      "                        if not entity == related:\n",
      "                            create_relationship(g,entity,related,onto)\n",
      " \n",
      "           \n",
      "    return g\n",
      "\n",
      "onto = Graph()\n",
      "onto.parse(\"../../code/resource-projects-etl/ontology.rdf\", format=\"xml\")\n",
      "\n",
      "\n",
      "#datasets = OrderedDict()\n",
      "#datasets['eiti-project-level'] = pd.read_csv('eiti-project-level.csv')\n",
      "\n",
      "datasets = OrderedDict()\n",
      "datasets['openoil-concessions'] = pd.read_csv('data/indonesia/3-openoil-concessions-indonesia.csv')\n",
      "\n",
      "\n",
      "for dataset in datasets:\n",
      "    print \"Mapping tags for \" + dataset\n",
      "    data = map_tags(datasets[dataset])\n",
      "    print \"Generating graph for \" + dataset\n",
      "    g = generate_graph(data, onto,dataset)\n",
      "    print \"Writing out dataset\"\n",
      "    g.serialize(format='turtle', destination=\"rdf/\"+dataset+\".ttl\")\n",
      "    print \"Written output to rdf/\"+dataset+\".ttl\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mapping tags for openoil-concessions\n",
        "Generating graph for openoil-concessions\n",
        "Writing out dataset"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Written output to rdf/openoil-concessions.ttl"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "print generate_project_identifier(\"Jubilee\")\n",
      "print generate_project_identifier(\"East Kalaman Fields\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "jubi-gzgzz3\n",
        "eaka-9qbtcx\n"
       ]
      }
     ],
     "prompt_number": 41
    }
   ],
   "metadata": {}
  }
 ]
}